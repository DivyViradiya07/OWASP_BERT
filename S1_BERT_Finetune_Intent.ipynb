{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae2fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bb1ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd1d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON files...\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3775 entries, 0 to 3774\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  3775 non-null   object\n",
      " 1   intent    3775 non-null   object\n",
      " 2   type      3775 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 88.6+ KB\n",
      "None\n",
      "\n",
      "DataFrame Head:\n",
      "                                            question  \\\n",
      "0  What does 'Broken Access Control' refer to in ...   \n",
      "1  What does 'Broken Access Control' refer to in ...   \n",
      "2  What does 'Broken Access Control' refer to in ...   \n",
      "3  What does 'Broken Access Control' refer to in ...   \n",
      "4  What does 'Broken Access Control' refer to in ...   \n",
      "\n",
      "                         intent                 type  \n",
      "0  define_broken_access_control  basic_understanding  \n",
      "1  define_broken_access_control  basic_understanding  \n",
      "2  define_broken_access_control  basic_understanding  \n",
      "3  define_broken_access_control  basic_understanding  \n",
      "4  define_broken_access_control  basic_understanding  \n",
      "\n",
      "Unique Intents:\n",
      "intent\n",
      "basic_understanding                   60\n",
      "vulnerability_identification          60\n",
      "prevention_methods                    53\n",
      "references                            50\n",
      "define_broken_access_control          50\n",
      "                                      ..\n",
      "prevent_regular_reviews                1\n",
      "explain_missing_api_access_control     1\n",
      "explain_least_privilege_principle      1\n",
      "explain_rate_limiting_mechanism        1\n",
      "common_logging_mistakes                1\n",
      "Name: count, Length: 3162, dtype: int64\n",
      "\n",
      "Unique Types:\n",
      "type\n",
      "prevention_methods              502\n",
      "basic_understanding             500\n",
      "example_scenarios               490\n",
      "vulnerability_identification    486\n",
      "technical_explanation           478\n",
      "statistics                      420\n",
      "references                      417\n",
      "proactive_suggestions           330\n",
      "probable_scenarios               82\n",
      "proactive                        30\n",
      "probable_issues                  20\n",
      "problems_that_might_arise        20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_json_data(file_paths):\n",
    "    \"\"\"\n",
    "    Loads and parses JSON files, recursively extracting 'question', 'type', and 'intent'.\n",
    "    This function is made more robust to handle varied nesting levels.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    def extract_qa_items(obj):\n",
    "        \"\"\"Recursively extracts Q&A items from a nested JSON object or list.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            if 'question' in obj and 'intent' in obj and 'type' in obj:\n",
    "                # Found a Q&A item\n",
    "                all_data.append({\n",
    "                    'question': obj['question'],\n",
    "                    'intent': obj['intent'],\n",
    "                    'type': obj['type']\n",
    "                })\n",
    "            else:\n",
    "                # Recurse into dictionary values\n",
    "                for key, value in obj.items():\n",
    "                    extract_qa_items(value)\n",
    "        elif isinstance(obj, list):\n",
    "            # Recurse into list elements\n",
    "            for item in obj:\n",
    "                extract_qa_items(item)\n",
    "\n",
    "    for file_path in file_paths:  # Changed from file_files to file_paths\n",
    "        try:\n",
    "            # Read file directly from filesystem\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                extract_qa_items(data)  # Start recursive extraction from the root of the JSON\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: {file_path} not found.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not decode JSON from {file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Define file paths\n",
    "file_paths = [\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A01_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A02_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A03_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A04_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A05_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A06_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A07_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A08_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A09_2021.json',\n",
    "    r'D:\\OWASP_BERT\\QA_Pairs\\Enhanced_QA\\A10_2021.json'\n",
    "]\n",
    "\n",
    "# Load data into a DataFrame\n",
    "print(\"Loading JSON files...\")\n",
    "df = load_json_data(file_paths)\n",
    "\n",
    "# Print debug info\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDataFrame Head:\")\n",
    "print(df.head())\n",
    "\n",
    "# Only try to access these columns if they exist\n",
    "if not df.empty:\n",
    "    if 'intent' in df.columns:\n",
    "        print(\"\\nUnique Intents:\")\n",
    "        print(df['intent'].value_counts())\n",
    "    else:\n",
    "        print(\"\\nNo 'intent' column found in the DataFrame.\")\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "    \n",
    "    if 'type' in df.columns:\n",
    "        print(\"\\nUnique Types:\")\n",
    "        print(df['type'].value_counts())\n",
    "    else:\n",
    "        print(\"\\nNo 'type' column found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347998bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2979 intents with only one occurrence.\n",
      "DataFrame shape after duplicating single-occurrence intents: (6754, 3)\n",
      "\n",
      "Unique Intents (after duplication):\n",
      "intent\n",
      "basic_understanding                   60\n",
      "vulnerability_identification          60\n",
      "prevention_methods                    53\n",
      "references                            50\n",
      "define_broken_access_control          50\n",
      "                                      ..\n",
      "explain_idor_technical                 2\n",
      "explain_missing_api_access_control     2\n",
      "explain_least_privilege_principle      2\n",
      "explain_rate_limiting_mechanism        2\n",
      "explain_force_browsing                 2\n",
      "Name: count, Length: 3162, dtype: int64\n",
      "\n",
      "Number of unique intents (all included, after duplication): 3162\n",
      "Example mapping (ID to Intent): [(0, 'CI_CD_for_component_management'), (1, 'SBOM_role_in_vulnerability_management'), (2, 'absence_of_abuse_case_modeling'), (3, 'absence_of_business_logic_validation'), (4, 'absence_of_logging_monitoring')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prepare Labels for Classification & Handle Single-Occurrence Intents\n",
    "# Identify intents with only one occurrence\n",
    "intent_counts = df['intent'].value_counts()\n",
    "single_occurrence_intents = intent_counts[intent_counts == 1].index\n",
    "\n",
    "print(f\"\\nFound {len(single_occurrence_intents)} intents with only one occurrence.\")\n",
    "\n",
    "# Duplicate rows for single-occurrence intents\n",
    "rows_to_duplicate = df[df['intent'].isin(single_occurrence_intents)]\n",
    "df_processed = pd.concat([df, rows_to_duplicate], ignore_index=True)\n",
    "\n",
    "print(f\"DataFrame shape after duplicating single-occurrence intents: {df_processed.shape}\")\n",
    "print(\"\\nUnique Intents (after duplication):\")\n",
    "print(df_processed['intent'].value_counts()) # Verify counts are now >= 2 for all\n",
    "\n",
    "# We will encode 'intent' labels into numerical IDs.\n",
    "# All intents, including those duplicated, will be included.\n",
    "label_encoder = LabelEncoder()\n",
    "df_processed['intent_id'] = label_encoder.fit_transform(df_processed['intent'])\n",
    "\n",
    "# Create a mapping from intent_id back to intent string and type string for inference\n",
    "id_to_intent = {i: intent for i, intent in enumerate(label_encoder.classes_)}\n",
    "# Create a dictionary to map each intent to its corresponding type\n",
    "intent_to_type = df_processed.set_index('intent')['type'].to_dict()\n",
    "\n",
    "print(f\"\\nNumber of unique intents (all included, after duplication): {len(label_encoder.classes_)}\")\n",
    "print(f\"Example mapping (ID to Intent): {list(id_to_intent.items())[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce1e0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 3377\n",
      "Validation set size: 3377\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split Data into Training and Validation Sets\n",
    "# IMPORTANT: Adjusted `test_size` to ensure it's greater than or equal to the number of unique classes,\n",
    "# allowing `stratify` to work correctly.\n",
    "train_df, val_df = train_test_split(df_processed, test_size=0.5, random_state=42, stratify=df_processed['intent'])\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a266b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully. Max length: 512\n",
      "\n",
      "Example from training dataset (first item):\n",
      "{'input_ids': tensor([    0,  2264,  8418,   109,   230,  9112,    29,   215,    25,   230,\n",
      "         9112,    12,  1549,     8,   230,  9112,    12,   401,  1225,   694,\n",
      "           77, 18999,   573,  3834, 43163, 27975,  6732,   116,     2,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(417)}\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Tokenization and Custom Dataset\n",
    "try:\n",
    "    # CHANGED: Using 'roberta-base' tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    print(f\"Tokenizer loaded successfully. Max length: {tokenizer.model_max_length}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    print(\"This might be due to network issues, missing required libraries, or corrupted cache.\")\n",
    "    print(\"Please ensure your environment has the necessary dependencies for RoBERTa-base.\")\n",
    "    print(\"If issue persists, try clearing your Hugging Face cache (usually at ~/.cache/huggingface).\")\n",
    "    exit() # Exit if tokenizer loading fails, as subsequent steps will also fail\n",
    "\n",
    "\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, questions, intent_ids, tokenizer):\n",
    "        self.encodings = tokenizer(questions.tolist(), truncation=True, padding=True, max_length=128)\n",
    "        self.intent_ids = intent_ids.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.intent_ids[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.intent_ids)\n",
    "\n",
    "train_dataset = IntentDataset(train_df['question'], train_df['intent_id'], tokenizer)\n",
    "val_dataset = IntentDataset(val_df['question'], val_df['intent_id'], tokenizer)\n",
    "\n",
    "print(f\"\\nExample from training dataset (first item):\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03f16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Divy\\AppData\\Local\\Temp\\ipykernel_9100\\121311494.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting intent classification model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2115' max='2115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2115/2115 2:31:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.082700</td>\n",
       "      <td>8.064827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.070500</td>\n",
       "      <td>8.077924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.052900</td>\n",
       "      <td>7.993279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>7.978410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.971400</td>\n",
       "      <td>7.953343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intent classification model training complete.\n",
      "\n",
      "Fine-tuned intent classifier model, tokenizer, label_encoder, and intent_to_type mapping saved to: ./fine_tuned_intent_classifier\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Fine-tuning the Model\n",
    "# CHANGED: Using 'roberta-base' for AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=len(label_encoder.classes_) # Number of unique intents (all included, after duplication)\n",
    ").to(device)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_intent_classifier',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4, # Keep batch size small for MX 550\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_intent_classifier',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=2, # Crucial for MX 550 with this model size\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\nStarting intent classification model training...\")\n",
    "trainer.train()\n",
    "print(\"\\nIntent classification model training complete.\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model_save_path = \"./fine_tuned_intent_classifier\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the label encoder classes and the intent_to_type mapping for later use\n",
    "import joblib\n",
    "joblib.dump(label_encoder, os.path.join(model_save_path, 'label_encoder.joblib'))\n",
    "with open(os.path.join(model_save_path, 'intent_to_type.json'), 'w') as f:\n",
    "    json.dump(intent_to_type, f)\n",
    "\n",
    "print(f\"\\nFine-tuned intent classifier model, tokenizer, label_encoder, and intent_to_type mapping saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ee951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Intent Classifier ---\n",
      "\n",
      "Question: \"What does 'Broken Access Control' refer to in the context of secure web applications?\"\n",
      "  Predicted Intent: vulnerability_identification\n",
      "  Inferred Type: vulnerability_identification\n",
      "\n",
      "Question: \"How to prevent Cross-Site Scripting?\"\n",
      "  Predicted Intent: vulnerability_identification\n",
      "  Inferred Type: vulnerability_identification\n",
      "\n",
      "Question: \"Explain the concept of IDOR.\"\n",
      "  Predicted Intent: vulnerability_identification\n",
      "  Inferred Type: vulnerability_identification\n",
      "\n",
      "Question: \"Why should applications implement secure fallback mechanisms for cryptographic failures?\"\n",
      "  Predicted Intent: vulnerability_identification\n",
      "  Inferred Type: vulnerability_identification\n",
      "\n",
      "Question: \"How does using strong cryptographic techniques to protect access tokens help?\"\n",
      "  Predicted Intent: vulnerability_identification\n",
      "  Inferred Type: vulnerability_identification\n",
      "\n",
      "Question: \"What are common weak encryption algorithms?\"\n",
      "  Predicted Intent: vulnerability_identification\n",
      "  Inferred Type: vulnerability_identification\n",
      "\n",
      "Question: \"Describe the impact of Broken Access Control on a system.\"\n",
      "  Predicted Intent: vulnerability_identification\n",
      "  Inferred Type: vulnerability_identification\n",
      "\n",
      "Stage 1: Intent classification training and testing complete with RoBERTa-base (all intents included, duplicated rares).\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Test the Fine-tuned Model (Inference Example)\n",
    "print(\"\\n--- Testing Intent Classifier ---\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_save_path).to(device)\n",
    "loaded_label_encoder = joblib.load(os.path.join(model_save_path, 'label_encoder.joblib'))\n",
    "with open(os.path.join(model_save_path, 'intent_to_type.json'), 'r') as f:\n",
    "    loaded_intent_to_type = json.load(f)\n",
    "\n",
    "def classify_intent(text, model, tokenizer, label_encoder, intent_to_type, device):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = label_encoder.inverse_transform([predicted_class_id])[0]\n",
    "    predicted_type = intent_to_type.get(predicted_intent, \"Unknown Type\")\n",
    "    return predicted_intent, predicted_type\n",
    "\n",
    "# Test cases\n",
    "test_questions = [\n",
    "    \"What does 'Broken Access Control' refer to in the context of secure web applications?\",\n",
    "    \"How to prevent Cross-Site Scripting?\",\n",
    "    \"Explain the concept of IDOR.\",\n",
    "    \"Why should applications implement secure fallback mechanisms for cryptographic failures?\",\n",
    "    \"How does using strong cryptographic techniques to protect access tokens help?\",\n",
    "    \"What are common weak encryption algorithms?\",\n",
    "    \"Describe the impact of Broken Access Control on a system.\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    intent, q_type = classify_intent(q, loaded_model, loaded_tokenizer, loaded_label_encoder, loaded_intent_to_type, device)\n",
    "    print(f\"\\nQuestion: \\\"{q}\\\"\")\n",
    "    print(f\"  Predicted Intent: {intent}\")\n",
    "    print(f\"  Inferred Type: {q_type}\")\n",
    "\n",
    "print(\"\\nStage 1: Intent classification training and testing complete with RoBERTa-base (all intents included, duplicated rares).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
